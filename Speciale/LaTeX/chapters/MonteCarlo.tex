In this chapter, we provide the theoretical foundation for Particle Markov Chain Monte Carlo (PMCMC). We follow the work of \cite{Andrieu}, \cite{Doucet}, and \cite{kroese2013handbook}.

Suppose we have a density $\pi_n(x_{1:n})$ for some $n$, where the normalizing constant is possibly unknown, that is
\begin{equation}
	\pi_n(x_{1:n})=\frac{\gamma_n(x_{1:n})}{Z_n}, \label{eq:density}
\end{equation}
where $\gamma_n(x_{1:n})$ is the unnormalized density and $Z_n$ is a normalizing constant
\begin{equation}
	Z_n=\int \gamma_n(x_{1:n})dx_{1:n}.	\label{eq:normalizing_constant}
\end{equation}
Let $X_{1:n}\sim \pi_n(x_{1:n})$ and suppose we generate $N$ i.i.d. samples $x_{1:n}^{(1)},x_{1:n}^{(2)},\dots,x_{1:n}^{(N)}$. We can then approximate $\pi_n(x_{1:n})$ by the empirical measure
\[
{\pi}_n^{\text{MC}}(x_{1:n})=\frac{1}{N} \sum_{i=1}^N \delta_{X_{1:n}^{(i)}}(x_{1:n}),
\]
and any marginal $\pi_n(x_k)$ as
\[
{\pi}_n^{\text{MC}}(x_k)=\frac{1}{N} \sum_{i=1}^N \delta_{X_{k}^{(i)}}(x_{k}).
\]
The expectation of any function $H_n: \mathcal{X}^n \to \mathbb{R}$ is given by
\[
I_n(H_n)\coloneq \mathbb{E}_{X_{1:n} \sim \pi_n}[H_n(X_{1:n})]=\int H_n(x_{1:n})\pi_n(x_{1:n})\, dx_{1:n},
\]
and we can estimate it by
\[
I_n^{\text{MC}}(H_n) \coloneq \int H_n(x_{1:n})\pi_n^{\text{MC}}(x_{1:n})dx_{1:n}=\frac{1}{N}\sum_{i=1}^NH_n(x_{1:n}^{(i)}).
\]
% Proof unbiased+variance?
However, this requires that we can sample from $\pi_n(x_{1:n})$, which often is not the case when it is a complex high-dimensional distribution. 
\section{Importance Sampling}
A way to solve this issue is to use importance sampling (IS). Here we introduce an importance density $q_n(x_{1:n})$ which we can sample from and such that 
\[
\pi_n(x_{1:n})>0 \implies q_n(x_{1:n})>0.
\]
For the remainder of this chapter, we let $X_{1:n} \sim q_n(x_{1:n})$. Suppose we generate $N$ i.i.d. samples $x_{1:n}^{(1)},x_{1:n}^{(2)},\dots,x_{1:n}^{(N)}$. 
To correct for the fact that we sample from $q_n$ we define the \emph{unnormalized weight} function
\[
w_n(x_{1:n}) \coloneq \frac{\gamma_n(x_{1:n})}{q_n(x_{1:n})}
\]
and define the \emph{normalized weight} function
\[
W_n^{(i)} \coloneq \frac{w_n(X_{1:n}^{(i)})}{\sum_{j=1}^N w_n(X_{i:n}^{(j)})}.
\]
From (\ref{eq:density}) and (\ref{eq:normalizing_constant}) we get
\begin{equation}
	\pi_n(x_{1:n})=\frac{w_n(x_{1:n})q_n(x_{1:n})}{Z_n},
\end{equation}
and
\begin{equation}
	Z_n=\int w_n(x_{1:n})q_n(x_{1:n})\, dx_{1:n}.
\end{equation}
We then define the IS estimators of respectively $\pi_n(x_{1:n})$ and $Z_n$ as
\begin{align}
	\widehat{\pi}_n(x_{1:n}) &= \sum_{i=1}^{N}W_n^{(i)} \delta_{X_{1:n}^{(i)}}(x_{1:n}), \label{eq:est_pi} \\
	\widehat{Z}_n &= \frac{1}{N}\sum_{i=1}^{N}w_n(X_{1:n}^{(i)}). \label{eq:est_Z}
\end{align}
Next, we will show what the relative variance of $\widehat{Z}_n$ is.
\begin{theorem}[Relative variance of $\Var(\widehat{Z}_n)$]
	\label{thm:rel_var_Z_IS}
	The relative variance of the IS estimate of the  normalizing constant $Z_n$ is given by
	\[
	\frac{\Var(\widehat{Z}_n)}{Z_n^2}=\frac{1}{N}\Bigl(\int \frac{\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-1\Bigr).
	\]
\end{theorem}
\begin{proof}
	The variance of $\widehat{Z}_n$ is 
	\begin{align*}
		\Var(\widehat{Z}_n)&=\frac{1}{N}\Var\bigl(w_n(X_{1:n})\bigr) \\
		&=\frac{1}{N}\EX[w_n^2(X_{1:n})]-(\EX[w_n(X_{1:n})])^2 \\
		&=\frac{1}{N}\Bigl(\int \frac{\gamma_n^2(x_{1:n})}{q_n^2(x_{1:n})} q_n(x_{1:n})\, dx_{1:n}-Z_n^2\Bigr) \\
		&=\frac{1}{N}\Bigl(\int \frac{\gamma_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-Z_n^2\Bigr) \\
		&=\frac{1}{N}\Bigl(\int \frac{Z_n^2\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-Z_n^2\Bigr) \\
		&=\frac{Z_n^2}{N}\Bigl(\int \frac{\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-1\Bigr).
	\end{align*}
	Dividing by $Z_n^2$ gives the result.
\end{proof}
\noindent Furthermore, we can also estimate $I_n(H_n)$ by
\[
I_n^{\text{IS}}(H_n) \coloneq \int H_n(x_{1:n})\widehat{\pi}(x_{1:n})\, dx_{1:n}=\sum_{i=1}^N W_n^{(i)}H_n(X_{1:n}^{(i)})=\frac{\frac{1}{N}\sum_{i=1}^{N}w_n(X_{1:n}^{(i)})H_n(X_{1:n}^{(i)})}{\frac{1}{N}\sum_{i=1}^{N}w_n(X_{1:n}^{(i)})}.
\]
Note, that the numerator is an unbiased estimate of $Z_nI_n$ and the denominator an unbiased estimate of $Z_n$. Thus, we have a ratio of unbiased estimates, which is not unbiased. However, it is still consistent, which follows by using the law of large numbers and properties of a.s. convergence. 


A natural choice for an importance density $q_n(x_{1:n})$ is one that minimizes the variance of $\widehat{Z}_n$. As shown in Theorem \ref{thm:min_var_IS}, this minimum variance is achieved when
\[
q_n(x_{1:n})=\pi_n(x_{1:n}).
\]
However, we cannot select this, as this was the reason we used IS in the first place. Nonetheless, this result indicates that the importance density should closely resemble the target density.

We could now sample from $\pi_n(x_{1:n})$ using the above method. However, to generate a sequence of samples for each $n$, we would have that each step would grow linearly in $n$, as generating samples from $\pi_{n+1}(x_{1:n+1})$ depends on the previous samples up to time $n$. This makes such an algorithm unfeasible in practice. 

\begin{theorem}[Minimum Variance of IS]
	\label{thm:min_var_IS}
	The variance \(\Var[\widehat{Z}_n]\) is minimized if
	\[
	q_n(x_{1:n}) = \pi_n(x_{1:n}) = \frac{\gamma_n(x_{1:n})}{Z_n}.
	\]
\end{theorem}
The proof is done by using Lagrange multiplier, and is inspired by \cite{min_var_IS}.
\begin{proof}
	By independence we have, 
	\[
		\Var[\widehat{Z}_n] = \frac{1}{N}\Var\left[w_n(X_{1:n})\right].
	\]
	We then have   
	\[
		\Var[w_n(X_{1:n})] = \mathbb{E}\left[\frac{\gamma_n(X_{1:n})^2}{q_n(X_{1:n})^2}\right] - Z_n^2
		= \int \frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n} - Z_n^2.
	\]
	Minimizing \(\Var[\widehat{Z}_n]\) is thus equivalent to minimizing
	\[
		J(q_n) = \int \frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n},
	\]
	subject to the constraint
	\[
		\int q_n(x_{1:n})\, dx_{1:n} = 1.
	\]
	We now introduce a Lagrange multiplier \(\lambda\) and form the Lagrangian
	\[
		L(q_n, \lambda) = \int \frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n} 
		+ \lambda \left(\int q_n(x_{1:n})\, dx_{1:n} - 1\right).
	\]
	Taking the functional derivative with respect to \(q_n(x_{1:n})\) and using chain rule we have
	\[
		\frac{\delta L}{\delta q_n(x_{1:n})} = -\frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})^2} + \lambda = 0.
	\]
	Solving for \(q_n(x_{1:n})\) yields
	\[
		q_n(x_{1:n}) = \frac{\gamma_n(x_{1:n})}{\sqrt{\lambda}}.
	\]
	Enforcing the normalization condition we get
	\[
		\int \frac{\gamma_n(x_{1:n})}{\sqrt{\lambda}}\, dx_{1:n} = 1 \quad\Longrightarrow\quad \frac{Z_n}{\sqrt{\lambda}} = 1,
	\]
	so that \(\sqrt{\lambda} = Z_n\). Therefore,
	\[
		q_n(x_{1:n}) = \frac{\gamma_n(x_{1:n})}{Z_n} = \pi_n(x_{1:n}).
	\]
	This completes the proof.
\end{proof}
\section{Sequential Importance Sampling}
Sequential Importance Sampling (SIS) builds upon the basic idea of IS by exploiting a Markov structure of the importance distribution. Instead of sampling the full trajectory at once, SIS extends the particle trajectories sequentially, updating the importance weights recursively. This leads to significant computational savings.

Formally, we let our importance distribution have a Markov structure, that is
\[
q_n(x_{1:n})=q_1(x_1)q_2(x_2\vert x_1)\dots q_n(x_k\vert x_{1:k-1}).
\]
Each element in the set $\{x_{1:t}\}$ we will refer to as a particle. We define for $n\geq 2$ the \emph{incremental importance weight} as
\[
\alpha_n(x_{1:n}) \coloneq \frac{\gamma_n(x_{1:n})}{\gamma_{n-1}(x_{1:n-1})q_n(x_n \vert x_{1:n-1})}.
\]
The unnormalized weights can then be written recursively as 
\begin{align}
	\begin{split}
		\label{eq:unnormalized_weights_recursive}
		w_n(x_{1:n})&=\frac{\gamma_n(x_{1:n})}{q_n(x_{1:n})} \\
		&=\frac{\gamma_{n-1}(x_{1:n-1})}{q_{n-1}(x_{1:n-1})}\frac{\gamma_n(x_{1:n})}{\gamma_{n-1}(x_{1:n-1})q_n(x_n\vert x_{1:n-1})} \\
		&=w_{n-1}(x_{1:n-1})\cdot \alpha_n(x_{1:n})  \\
		&=w_1(x_1) \prod_{k=2}^n \alpha_k(x_{1:k}).
	\end{split}
\end{align}
Summarizing, the SIS method is described in \hyperref[algo:SIS]{Algorithm \ref*{algo:SIS}}.

\begin{algorithm}[H]
	\caption{Sequential Importance Sampling (SIS)}
	\label{algo:SIS}
	\begin{algorithmic}[1]
		\State Generate particles \(x_1^{(i)}\) from  \(q_1(x_1)\) for \( i = 1, \dots, N \)
		\For{each time step \(n = 2, \dots, T\)}
		\For{each particle \( i = 1, \dots, N \)}
		\State Generate particles \(x_n^{(i)}\) from \(q_n(x_n\vert x_{1:n-1}^{(i)})\) 
		\State Compute the incremental importance weight: 
		\[
		\alpha_n^{(i)} = \frac{\gamma_n(x_{1:n}^{(i)})}{\gamma_{n-1}(x_{1:n-1}^{(i)}) q_n(x_n^{(i)} \vert x_{1:n-1}^{(i)})} 
		\] 
		\State Update the particle weight: \( w_n^{(i)} = w_{n-1}^{(i)} \cdot \alpha_n^{(i)} \)
		\EndFor
		\State Normalize the weights: \(W_n^{(i)} \leftarrow \frac{w_n^{(i)}}{\sum_{i=1}^N w_n^{(i)}} \)
		\EndFor
	\end{algorithmic}
\end{algorithm}

However, this method has a severe drawback, that the relative estimated variance $\Var(\widehat{Z}_n)/Z_n^2$ increases exponentially in $n$ even in simple examples. Example \ref{exa:rel_var_IS_explodes} illustrates this issue.
\begin{example}
	\label{exa:rel_var_IS_explodes}
	Consider the case where $\mathcal{X}=\mathbb{R}$ and let the density $\pi_n(x_{1:n})$ be given by
	\[
	\pi_n(x_{1:n})=\prod_{k=1}^n \pi_n(x_k)=\prod_{k=1}^n N(x_k; 0, 1).
	\]
	Thus, the unnormalized density $\gamma_n(x_{1:n})$ is given by
	\[
	\gamma_n=\prod_{k=1}^n \exp\Bigl(-\frac{x_k^2}{2}\Bigr),
	\]
	and the normalizing constant $Z_n$ is
	\[
	Z_n=(2\pi)^{n/2}.
	\]
	Ignoring that we could easily sample from $\pi_n$, we select the importance distribution $q_n$ to sample from as
	\[q_n(x_{1:n})=\prod_{k=1}^n \pi_n(x_k)=\prod_{k=1}^n N(x_k; 0, \sigma^2)\]
	and we are interested in estimating $Z_n$. Recall from Theorem \ref{thm:rel_var_Z_IS} that the relative variance of $\widehat{Z}_n$ is given by
	\[
	\frac{\Var(\widehat{Z}_n)}{Z_n^2}=\frac{1}{N}\Bigl(\int \frac{\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-1\Bigr).
	\]
	In our case this factors over the coordinates and we have
	\begin{align*}
		\int \frac{\pi_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n}&=\prod_{k=1}^{n}\int \frac{1/(2\pi)\exp(-x^2)}{1/\sqrt{2\pi\sigma^2}\exp(-x^2/(2\sigma^2))}\, dx_{1:k} \\
		&=\biggl[\frac{\sqrt{2\pi\sigma^2}}{2\pi}\int \exp\Bigl(-x^2+\frac{x^2}{2\sigma^2}\Bigr)\, dx \biggr]^n \\
		&=\biggl[\frac{\sqrt{2\pi\sigma^2}}{2\pi}\int \exp\Bigl(-\Bigl(1-\frac{1}{\sigma^2}\Bigr)x^2\Bigr)\, dx \biggr]^n.
	\end{align*}
	The integral is finite if and only if $1-1/(2\sigma^2)>0$, that is $\sigma^2>1/2$. In this case, it is equal to 
	\[
	\int_{-\infty}^{\infty} \exp\Bigl(-\Bigl(1-\frac{1}{\sigma^2}\Bigr)x^2\Bigr)\, dx=\sqrt{\frac{\pi}{1-1/(2\sigma^2)}}.
	\]
	Thus, for $\sigma^2>1/2$ we have
	\begin{align*}
		\int \frac{\pi_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n} &=\biggl(\frac{\sqrt{2\pi\sigma^2}}{2\pi} \sqrt{\frac{\pi}{1-1/(2\sigma^2)}}\biggr)^n \\
		&=\biggl(\sqrt{\frac{\sigma^4}{2\sigma^2-1}}\biggr)^n \\
		&=\Bigl(\frac{\sigma^4}{2\sigma^2-1}\Bigr)^{n/2}.
	\end{align*}
	Finally, we can conclude that for $\sigma^2>1/2$ the relative variance becomes $\Var[\widehat{Z}_n]<\infty$ and 
	\[
	\frac{\Var[\widehat{Z}_n]}{Z_n^2}=\frac{1}{N}\biggl[\Bigl(\frac{\sigma^4}{2\sigma^2-1}\Bigr)^{n/2}-1\biggr].
	\]
	Note that for any $1/2<\sigma^2\neq 1$ we have that $\sigma^4/(2\sigma^2-1)>1$, and thus the variance increases exponentially with $n$. 
	
	For example, choosing $\sigma^2=1.2$ then we have a reasonably good importance distribution as $q_k(x_k) \approx \pi_n(x_k)$. However,  
	\[
	N\Var[\widehat{Z}_n]/Z_n^2\approx (1.103)^{n/2},
	\]
	which for $n=1000$ is roughly equal to $1.9\cdot 10^{21}$. So we would need to use $N\approx 2\cdot 10^{23}$ particles to obtain a relative variance of $0.01$.
\end{example}



\section{Resampling}
%As the sequential importance sampling (SIS) procedure progresses, a well-known issue known as \emph{weight degeneracy} can occur. Over time, the importance weights \( \{w_n^{(i)}\}_{i=1}^N \) tend to become highly unequal: most of the weight concentrates on a small subset of particles while the remainder contributes very little to the approximation of the target density. This degeneracy not only reduces the algorithm's efficiency but also adversely affects the variance of estimators.
Over time, many particles receive negligible weight, leading to a situation where only a few particles dominate the estimate. This phenomenon, known as weight degeneracy, can degrade the performance of the estimator.
The idea for resampling is to get rid of particles with low weights with a high probability, so the focus is spent on high-probability regions instead of carrying forward particles with very low weights. This typically reduces the variance, see for instance Example \ref{exa:rel_var_IS_explodes_cont}. 
% Also referred to as Bootstrap filter.

Formally, the IS approximation $\widehat{\pi}_n(x_{1:n})$ of the target distribution $\pi_n(x_{1:n})$ is based on weighted samples drawn from $q_n(x_{1:n})$. Consequently, these samples are not distributed according to  $\pi_n(x_{1:n})$. To obtain samples from $\pi_n(x_{1:n})$, we can resample $X_{1:n}^{(i)}$ with probability $W_n^{(i)}$; that is, we resample from our approximation $\widehat{\pi}_n(x_{1:n})$ to approximately obtain samples from $\pi_n(x_{1:n})$. In practice, to obtain $N$ samples, we can select $N_n^{(i)}$ offspring for each particle $X_{1:n}^{(i)}$ such that 
\[
	N_n^{(1:n)}=(N_n^{(1)},\dots, N_n^{(N)}) \sim \text{Multinomial}(N, W_n^{(1:N)}).
\]
Using the recursive structure of the unnormalized weights from (\ref{eq:unnormalized_weights_recursive}) a natural way to estimate 
$Z_n$ is to define 
\[
	\widetilde{Z}_1\coloneq \frac{1}{N}\sum_{i=1}^N w_1(x_1^{i}),
\]
and for $n\geq 2$ estimate $Z_n$ recursively by 
\begin{equation}
	\widetilde{Z}_n \coloneq\widetilde{Z}_{n-1}{\alpha}_n^{\text{MC}}, \label{eq:est_Z_SIS}
\end{equation}
where ${\alpha}_n^{\text{MC}}$ is the standard MC estimate of $\alpha_n$. We can still estimate $\pi_n(x_{1:n})$ by (\ref{eq:est_pi}). 

A generic SISR algorithm is given in \hyperref[algo:SISR]{Algorithm \ref*{algo:SISR}}.
\begin{algorithm}[H]
	\caption{Sequential Importance Sampling with Resampling (SISR)}
	\label{algo:SISR}
	\begin{algorithmic}[1]
		\State Generate particles \(x_1^{(i)}\) from \( q_1(x_1)\) for \( i = 1, \dots, N \)
		\For{each time step \(n = 2, \dots, T\)}
		\For{each particle \( i = 1, \dots, N \)}
		\State Generate particles \(x_n^{(i)}\) from \(q_n(x_n\vert x_{1:n-1}^{(i)})\) 
		\State Compute the incremental importance weight: 
		\[
		\alpha_n^{(i)} = \frac{\gamma_n(x_{1:n}^{(i)})}{\gamma_{n-1}(x_{1:n-1}^{(i)}) q_n(x_n^{(i)} \vert x_{1:n-1}^{(i)})} 
		\] 
		\State Update the particle weight: \( w_n^{(i)} = w_{n-1}^{(i)} \cdot \alpha_n^{(i)} \)
		\EndFor
		\State Normalize the weights: \(W_n^{(i)} \leftarrow \frac{w_n^{(i)}}{\sum_{i=1}^N w_n^{(i)}} \)
		\State Draw \(N\) indices \(\{a^{(i)}\}_{i=1}^N\) from \(\{1,\dots,N\}\) according to the probabilities \(\{W_n^{(i)}\}\).
		\State Set \( X_{1:n}^{(i)} \leftarrow X_{1:n}^{(a^{(i)})} \) for all \( i \).
		\State Reset weights: $w_n^{(i)}=1$.
		\EndFor
	\end{algorithmic}
\end{algorithm}
Now we provide a result similar to Theorem \ref{thm:rel_var_Z_IS} for the case with resampling. 
\begin{comment}
	Thus, we can approximate $\widehat{\pi}_n(x_{1:n})$ by the resampled empirical measure 
	\[
	\overline{\pi}_n(x_{1:n}) \coloneq \frac{1}{N}\sum_{i=1}^{N} N_n^{(i)} \delta_{X_{1:n}^{(i)}}(x_{1:n}).
	\]
\end{comment}
\begin{comment}
	We can also estimate the \emph{incremental normalizing constant} \(Z_k/Z_{k-1}\) by
	\[
	\widehat{\frac{Z_k}{Z_{k-1}}}=\frac{1}{N} \sum_{i=1}^N \alpha_k(x_{1:k}^{(i)}).
	\]
	This will be useful in the sketch of the proof of Theorem \ref{thm:rel_asym_var_resampling}.
\end{comment}

\begin{theorem}[Relative Asymptotic Variance with Resampling]
	\label{thm:rel_asym_var_resampling}
	The relative asymptotic variance of the IS estimate of the normalizing constant \(Z_n\) with resampling at every time step is
	\[
	\frac{\Var(\widetilde{Z}_n)}{Z_n^2}=\frac{1}{N}\left[\left(\int 	\frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right)+\sum_{k=2}^n\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right)\right].
	\]
\end{theorem}

A formal proof is omitted for brevity (it can be derived using the Feynman–Kac framework; see, e.g., \cite{moral2004feynman}). 

Notably, comparing Theorem \ref{thm:rel_asym_var_resampling} with Theorem \ref{thm:rel_var_Z_IS} reveals that while resampling introduces additional variance, the resampling has a resetting property of the systems. Thus, we get that the associated errors accumulate linearly rather than multiplicatively. This linear propagation can offer enhanced stability, especially in high-dimensional settings where multiplicative error growth might otherwise lead to significant degradation in performance. We continue Example \ref{exa:rel_var_IS_explodes} using Theorem \ref{thm:rel_asym_var_resampling} to highlight this.
\begin{comment}
	Below we provide a proof sketch, where the handling of error propagation is ignored.
	\begin{proof}[Proof sketch]
		We begin by writing the estimator \(\widehat{Z}_n\) as a product of the incremental estimators:
		\[
		\widehat{Z}_n = \widehat{Z}_1 \prod_{k=2}^n \widehat{\frac{Z_k}{Z_{k-1}}}.
		\]
		For notational convenience, define for \(k=1\)
		\[
		\widehat{A}_1 = \widehat{Z}_1,
		\]
		and for \(k\ge2\)
		\[
		\widehat{A}_k = \widehat{\frac{Z_k}{Z_{k-1}}},
		\]
		with the corresponding true values
		\[
		A_1 = Z_1,\quad \text{and for } k\ge2,\quad A_k = \frac{Z_k}{Z_{k-1}}.
		\]
		We now express each estimator in a \emph{relative error} form:
		\[
		\widehat{A}_k = A_k \, (1+\epsilon_k),
		\]
		where
		\[
		\epsilon_1 = \frac{\widehat{Z}_1-Z_1}{Z_1},\quad \text{and for } k\ge 2,\quad \epsilon_k = \frac{\widehat{\frac{Z_k}{Z_{k-1}}}-\frac{Z_k}{Z_{k-1}}}{\frac{Z_k}{Z_{k-1}}}.
		\]
		Thus, we can rewrite $\widehat{Z}_n$ as	
		\[
		\widehat{Z}_n = Z_1\prod_{k=2}^n \frac{Z_k}{Z_{k-1}} \prod_{k=1}^n (1+\epsilon_k) = Z_n \prod_{k=1}^n (1+\epsilon_k).
		\]
		Taking logarithms gives
		\[
		\log \widehat{Z}_n = \log Z_n + \sum_{k=1}^n \log (1+\epsilon_k).
		\]
		For small \(\epsilon_k\), we apply the first-order Taylor expansion
		\[
		\log (1+\epsilon_k) \approx \epsilon_k,
		\]
		so that
		\[
		\log \widehat{Z}_n \approx \log Z_n + \sum_{k=1}^n \epsilon_k.
		\]
		Ignoring the error propagation \(\epsilon_k\) we have that 
		\[
		\Var\Bigl(\log \widehat{Z}_n\Bigr) \approx \sum_{k=1}^n \Var(\epsilon_k).
		\]	
		At time \(k=1\), the IS estimator is 
		\[
		\widehat{Z}_1 = \frac{1}{N}\sum_{i=1}^N w_1^{(i)}, \quad \text{with } w_1(x_1)=\frac{\gamma_1(x_1)}{q_1(x_1)},
		\]
		and its variance is
		\[
		\Var_{\text{ISR}}[\widehat{Z}_1] = \frac{1}{N}\left(\int \frac{\gamma_1^2(x_1)}{q_1(x_1)}\,dx_1-Z_1^2\right)
		=\frac{Z_1^2}{N}\left(\int \frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right).
		\]
		Since \(\epsilon_1 = (\widehat{Z}_1-Z_1)/Z_1\), it follows that
		\[
		\Var(\epsilon_1) = \frac{1}{N}\left(\int \frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right).
		\]
		
		For \(k\geq 2\), note that with resampling at time \(k-1\) the particles \(X_{1:k-1}^{(i)}\) are i.i.d. draws from (approximately) \(\pi_{k-1}\). The variance of the incremental estimator is then
		\[
		\Var_\text{ISR}\left[\widehat{\frac{Z_k}{Z_{k-1}}}\right]
		=\frac{1}{N}\left(\int \frac{\gamma_k^2(x_{1:k})}{\gamma_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-\left(\frac{Z_k}{Z_{k-1}}\right)^2\right).
		\]
		Using the identity \(\gamma_k(x_{1:k}) = Z_k\,\pi_k(x_{1:k})\), we can rewrite this as
		\[
		\Var_\text{ISR}\left[\widehat{\frac{Z_k}{Z_{k-1}}}\right]
		=\frac{Z_k^2}{N\,Z_{k-1}}\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right).
		\]
		Thus, for \(k\ge 2\), since \(\epsilon_k\) is the relative error in estimating \(Z_k/Z_{k-1}\), we have
		\[
		\Var(\epsilon_k) = \frac{1}{N}\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right).
		\]
		Therefore, the total variance on the log scale is approximately
		\[
		\Var\Bigl(\log \widehat{Z}_n\Bigr) \approx \frac{1}{N}\left[\left(\int \frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right)
		+\sum_{k=2}^n\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right)\right].
		\]
		
		Finally, we apply the delta method to transfer the variance from the logarithmic scale back to the original scale. Since for the function \(g(x)=\exp(x)\) we have \(g'(\log Z_n)=Z_n\), the delta method implies
		\[
		\Var\Bigl(\log \widehat{Z}_n\Bigr) \approx \frac{\Var(\widehat{Z}_n)}{Z_n^2},
		\]
		and hence the asymptotic relative variance is
		\[
		\frac{\Var(\widehat{Z}_n)}{Z_n^2}= \frac{1}{N}\left[\left(\int \frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right)
		+\sum_{k=2}^n\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right)\right],
		\]
		as desired.
	\end{proof}
\end{comment} 
\begin{example}[Example \ref{exa:rel_var_IS_explodes} continued]
	\label{exa:rel_var_IS_explodes_cont}
	Using Theorem \ref{thm:rel_asym_var_resampling} and using derivation done in Example \ref{exa:rel_var_IS_explodes} we have that it is finite for $\sigma^2>1/2$ and the relative variance using resampling at every time step is approximately equal to
	\begin{align*}
		\frac{\Var(\widetilde{Z}_n)}{Z_n^2}&\approx\frac{1}{N}\left[\left(\int 	\frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right)+\sum_{k=2}^n\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right)\right] \\
		&=\frac{n}{N}\left[\left(\frac{\sigma^4}{2\sigma^2-1}^{1/2}\right)-1\right]
	\end{align*}
	which is linear in $n$ in contrast to the exponential growth of the IS estimate of the relative variance. If we again select $\sigma^2=1.2$ then to obtain a relative variance of $0.01$ we only need $N\approx 10^4$ particles instead of the $N\approx 2\cdot 10^{23}$ particles that were needed for the IS estimate to obtain the same precision. That is, we obtain an improvement by 19 orders of magnitude.
\end{example} 
This setup favors SMC massively since the density $\pi_n(x_{1:n})$) factorizes. A more realistic example is given in Example \ref{exa:weight degeneracy}.

\subsection{Reducing variance of Resampling}
While resampling helps mitigate weight degeneracy by focusing computational effort on high-probability regions, it introduces additional variance into the algorithm. In this section, we describe two distinct techniques that can reduce this extra variance, thereby improving the overall efficiency of the SISR algorithm. Notably, these approaches are complementary and can be applied simultaneously.

We can reduce the variance introduced during resampling by leveraging sampling concepts such as stratified sampling. In stratified sampling, the interval $[0,1]$ is divided into $K$ equal strata, and one uniform random number is drawn from each subinterval. Formally, for $i=1,\dots,K$, we draw 
\[
u_i \sim \text{Uniform}\left(\frac{i-1}{K}, \frac{i}{K}\right).
\]
Each $u_i$ is then used to select a particle based on the cumulative normalized weights.

Another way to reduce the variance of the SISR algorithm is to only do the resampling step when we have many particles with low weights. We will call this Sequential Importance Sampling with Adaptive Resampling (SISAR).
A common metric used to decide when to trigger a resampling step is the \emph{effective sample size} (ESS), first introduced by \cite{Liu}. The ESS at time $n$ is defined as
\[
\text{ESS}_n \coloneq \frac{1}{\sum_{i=1}^N \left(W_n^{(i)}\right)^2}\,.
\]
ESS can take a value between $1$ and $N$. When the ESS falls below a predetermined threshold, $N_\tau$, often chosen as $N_\tau=N/2$, it is an indication that most of the weight is carried by only a few particles, and resampling is then warranted.
%\cite{Doucet}.	

\subsection{Resampling within SMC}
In the context of the SIS algorithm, resampling is integrated as an additional step to refresh the particle set, given that the resampling condition is met. A generic algorithm incorporating adaptive resampling is described in \hyperref[algo:SISAR]{Algorithm \ref*{algo:SISAR}}. 

\begin{algorithm}[H]
	\caption{Sequential Importance Sampling with Adaptive Resampling (SISAR)}
	\label{algo:SISAR}
	\begin{algorithmic}[1]
		\State Generate particles \(x_1^{(i)}\) from \( q_1(x_1)\) for \( i = 1, \dots, N \)
		\For{each time step \(n = 2, \dots, T\)}
		\For{each particle \( i = 1, \dots, N \)}
		\State Generate particles \(x_n^{(i)}\) from \(q_n(x_n\vert x_{1:n-1}^{(i)})\) 
		\State Compute the incremental importance weight: 
		\[
		\alpha_n^{(i)} = \frac{\gamma_n(x_{1:n}^{(i)})}{\gamma_{n-1}(x_{1:n-1}^{(i)}) q_n(x_n^{(i)} \vert x_{1:n-1}^{(i)})} 
		\] 
		\State Update the particle weight: \( w_n^{(i)} = w_{n-1}^{(i)} \cdot \alpha_n^{(i)} \)
		\EndFor
		\State Normalize the weights: \(W_n^{(i)} \leftarrow \frac{w_n^{(i)}}{\sum_{i=1}^N w_n^{(i)}} \)
		\If{Resampling condition is met}
		\State Draw \(N\) indices \(\{a^{(i)}\}_{i=1}^N\) from \(\{1,\dots,N\}\) according to the probabilities \(\{W_n^{(i)}\}\).
		\State Set \( x_{1:n}^{(i)} \leftarrow x_{1:n}^{(a^{(i)})} \) for all \( i \).
		\State Reset weights: $w_n^{(i)}=1$.
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}
\noindent Again, we can at any step $n$ estimate $\pi_n(x_{1:n})$ and $Z_n$ by respectively (\ref{eq:est_pi}) and (\ref{eq:est_Z_SIS}).

\begin{example}[Example with weight degeneracy?]
	\label{exa:weight degeneracy}
	\todo{Make example}
	\begin{comment}
		We resort to estimating the variance using Monte Carlo in this example. We implement the SIS, SISR, and SISAR algorithms in R. To avoid overflow we define
		\[
		\widetilde{Z}_n=\frac{\widehat{Z}_n}{Z_n},
		\]
		and we then estimate the variance of $\widetilde{Z}_n$ using Monte Carlo, since
		\[
		\Var(\widetilde{Z}_n)=\Var\Bigl(\frac{\widehat{Z}_n}{Z_n}\Bigr)=\frac{\Var(\widetilde{Z}_n)}{Z_n^2}.
		\]
		The code can be found at \todo{Insert GitHub link for code }
	\end{comment}
\end{example}