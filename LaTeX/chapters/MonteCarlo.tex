In this chapter, we provide the theoretical foundation for \gls*{PMCMC}. We follow the work of \cite{Andrieu}, \cite{Doucet}, and \cite{kroese2013handbook}.

Let $\mu$ be a $\sigma$-finite reference measure on the space $\mathcal{X}$ where each $x_i$ takes values, and denote by $\mu^{\otimes n}$ the corresponding product measure on $\mathcal{X}^n$. Let $x_{1:n} = (x_1, x_2, \dots, x_n)$ and suppose we have a density $\pi_n(x_{1:n})$ (with respect to $\mu^{\otimes n}$) given by
\begin{equation}
	\pi_n(x_{1:n}) = \frac{\gamma_n(x_{1:n})}{Z_n}, \label{eq:density}
\end{equation}
where $\gamma_n(x_{1:n})$ is the unnormalized density and the normalizing constant is defined as
\[
Z_n = \int_{\mathcal{X}^n} \gamma_n(x_{1:n})\, d\mu^{\otimes n}(x_{1:n}).
\]
In many applications and for notational simplicity, we for the rest of this chapter let $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mu$ be the Lebesgue measure, in which case  $Z_n$ is given by
\begin{equation}
	Z_n = \int \gamma_n(x_{1:n})\, dx_{1:n}.
	\label{eq:normalizing_constant}
\end{equation}

Let $X_{1:n}\sim \pi_n(x_{1:n})$ and suppose we generate $N$ i.i.d. samples $x_{1:n}^{(1)},x_{1:n}^{(2)},\dots,x_{1:n}^{(N)}$. We can then approximate $\pi_n(x_{1:n})$ by the empirical measure
\[
{\pi}_n^{\text{MC}}(x_{1:n})=\frac{1}{N} \sum_{i=1}^N \delta_{X_{1:n}^{(i)}}(x_{1:n}),
\]
where $\delta_{X_{1:n}^{(i)}}(x_{1:n})$ denotes the Dirac measure centered at $X_{1:n}^{(i)}$. We can also approximate any marginal $\pi_n(x_k)$ as
\[
{\pi}_n^{\text{MC}}(x_k)=\frac{1}{N} \sum_{i=1}^N \delta_{X_{k}^{(i)}}(x_{k}).
\]
The expectation of any function $H_n: \mathcal{X}^n \to \mathbb{R}$ is given by
\[
I_n(H_n)\coloneq \mathbb{E}_{X_{1:n} \sim \pi_n}[H_n(X_{1:n})]=\int H_n(x_{1:n})\pi_n(x_{1:n})\, dx_{1:n},
\]
and by the \gls*{LLN} we can estimate it by
\[
I_n^{\text{MC}}(H_n) \coloneq \int H_n(x_{1:n})\pi_n^{\text{MC}}(x_{1:n})dx_{1:n}=\frac{1}{N}\sum_{i=1}^NH_n(x_{1:n}^{(i)}).
\]
However, this requires that we can sample from $\pi_n(x_{1:n})$, which often is not the case when it is a complex high-dimensional distribution. 
\section{Importance Sampling}
A way to solve this issue is to use \gls*{IS}. Here we introduce an importance density $q_n(x_{1:n})$ which we can sample from and such that 
\[
\pi_n(x_{1:n})>0 \implies q_n(x_{1:n})>0.
\]
For the remainder of this chapter, we let 
\[
	X_{1:n} \sim q_n(x_{1:n}).
\]
Suppose we generate $N$ i.i.d. samples $x_{1:n}^{(1)},x_{1:n}^{(2)},\dots,x_{1:n}^{(N)}$. 
To correct for the fact that we sample from $q_n$ we define the \emph{unnormalized weight} function
\[
w_n(x_{1:n}) \coloneq \frac{\gamma_n(x_{1:n})}{q_n(x_{1:n})}
\]
and define the \emph{normalized weight} function
\[
W_n^{(i)} \coloneq \frac{w_n(X_{1:n}^{(i)})}{\sum_{j=1}^N w_n(X_{i:n}^{(j)})}.
\]
From \cref{eq:density} and \cref{eq:normalizing_constant} we get
\begin{equation}
	\pi_n(x_{1:n})=\frac{w_n(x_{1:n})q_n(x_{1:n})}{Z_n},
\end{equation}
and
\begin{equation}
	Z_n=\int w_n(x_{1:n})q_n(x_{1:n})\, dx_{1:n}.
\end{equation}
We then define the \gls*{IS} estimators of respectively $\pi_n(x_{1:n})$ and $Z_n$ as
\begin{align}
	\widehat{\pi}_n(x_{1:n}) &= \sum_{i=1}^{N}W_n^{(i)} \delta_{X_{1:n}^{(i)}}(x_{1:n}), \label{eq:est_pi} \\
	\widehat{Z}_n &= \frac{1}{N}\sum_{i=1}^{N}w_n(X_{1:n}^{(i)}). \label{eq:est_Z}
\end{align}
Next, we will show what the relative variance of $\widehat{Z}_n$ is.
\begin{theorem}[Relative variance of $\Var(\widehat{Z}_n)$]
	\label{thm:rel_var_Z_IS}
	The relative variance of the \gls*{IS} estimate of the  normalizing constant $Z_n$ is given by
	\[
	\frac{\Var(\widehat{Z}_n)}{Z_n^2}=\frac{1}{N}\Bigl(\int \frac{\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-1\Bigr).
	\]
\end{theorem}
\begin{proof}
	The variance of $\widehat{Z}_n$ is 
	\begin{align*}
		\Var(\widehat{Z}_n)&=\frac{1}{N}\Var\bigl(w_n(X_{1:n})\bigr) \\
		&=\frac{1}{N}\Bigl(\EX[w_n^2(X_{1:n})]-\EX[w_n(X_{1:n})]^2\Bigr) \\
		&=\frac{1}{N}\Bigl(\int \frac{\gamma_n^2(x_{1:n})}{q_n^2(x_{1:n})} q_n(x_{1:n})\, dx_{1:n}-Z_n^2\Bigr) \\
		&=\frac{1}{N}\Bigl(\int \frac{\gamma_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-Z_n^2\Bigr) \\
		&=\frac{1}{N}\Bigl(\int \frac{Z_n^2\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-Z_n^2\Bigr) \\
		&=\frac{Z_n^2}{N}\Bigl(\int \frac{\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-1\Bigr).
	\end{align*}
	Dividing by $Z_n^2$ gives the result.
\end{proof}
\noindent Furthermore, we can also estimate $I_n(H_n)$ by
\[
I_n^{\text{IS}}(H_n) \coloneq \int H_n(x_{1:n})\widehat{\pi}(x_{1:n})\, dx_{1:n}=\sum_{i=1}^N W_n^{(i)}H_n(X_{1:n}^{(i)})=\frac{\frac{1}{N}\sum_{i=1}^{N}w_n(X_{1:n}^{(i)})H_n(X_{1:n}^{(i)})}{\frac{1}{N}\sum_{i=1}^{N}w_n(X_{1:n}^{(i)})}.
\]
Note, that the numerator is an unbiased estimate of $Z_nI_n(H_n)$, since
\begin{align*}
	\EX\left[\frac{1}{N}\sum_{i=1}^{N} w_n(X_{1:n}^{(i)})H_n(X_{1:n}^{(i)})\right] &= \EX\Big[w_n(X_{1:n})H_n(X_{1:n})\Big] \\
	&=\int w_n(x_{1:n})H_n(x_{1:n})q(x_{1:n})\, dx_{1:n} \\
	&=\int H_n(x_{1:n})\gamma_n(x_{1:n})\, dx_{1:n} \\
	&=Z_n I_n(H_n).
\end{align*}
Similar calculations gives that the denominator is an unbiased estimate of $Z_n$. Thus, we have a ratio of unbiased estimates, which is not unbiased. However, it is still consistent, which follows by using the \gls*{LLN} and properties of a.s. convergence. 


A natural choice for an importance density $q_n(x_{1:n})$ is one that minimizes the variance of $\widehat{Z}_n$. As shown in \cref{thm:min_var_IS}, this minimum variance is achieved when
\[
q_n(x_{1:n})=\pi_n(x_{1:n}).
\]
However, we cannot select this, as this was the reason we used \gls*{IS} in the first place. Nonetheless, this result indicates that the importance density should closely resemble the target density.

We could now sample from $\pi_n(x_{1:n})$ using the above method. However, to generate a sequence of samples for each $n$, we would have that each step would grow linearly in $n$, as generating samples from $\pi_{n+1}(x_{1:n+1})$ depends on the previous samples up to time $n$. This makes such an algorithm unfeasible in practice. 

\begin{theorem}[Minimum Variance of IS]
	\label{thm:min_var_IS}
	The variance \(\Var[\widehat{Z}_n]\) is minimized if
	\[
	q_n(x_{1:n}) = \pi_n(x_{1:n}) = \frac{\gamma_n(x_{1:n})}{Z_n}.
	\]
\end{theorem}
The proof is done by using Lagrange multiplier, and is inspired by \cite{min_var_IS}.
\begin{proof}
	By independence we have, 
	\[
	\Var[\widehat{Z}_n] = \frac{1}{N}\Var\left[w_n(X_{1:n})\right].
	\]
	We then have   
	\[
	\Var[w_n(X_{1:n})] = \mathbb{E}\left[\frac{\gamma_n(X_{1:n})^2}{q_n(X_{1:n})^2}\right] - Z_n^2
	= \int \frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n} - Z_n^2.
	\]
	Minimizing \(\Var[\widehat{Z}_n]\) is thus equivalent to minimizing
	\[
	J(q_n) = \int \frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n},
	\]
	subject to the constraint
	\[
	\int q_n(x_{1:n})\, dx_{1:n} = 1.
	\]
	We now introduce a Lagrange multiplier \(\lambda\) and form the Lagrangian
	\[
	L(q_n, \lambda) = \int \frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n} 
	+ \lambda \left(\int q_n(x_{1:n})\, dx_{1:n} - 1\right).
	\]
	Taking the functional derivative with respect to \(q_n(x_{1:n})\) and using chain rule we have
	\[
	\frac{\delta L}{\delta q_n(x_{1:n})} = -\frac{\gamma_n(x_{1:n})^2}{q_n(x_{1:n})^2} + \lambda = 0.
	\]
	Solving for \(q_n(x_{1:n})\) yields
	\[
	q_n(x_{1:n}) = \frac{\gamma_n(x_{1:n})}{\sqrt{\lambda}}.
	\]
	Enforcing the normalization condition we get
	\[
	\int \frac{\gamma_n(x_{1:n})}{\sqrt{\lambda}}\, dx_{1:n} = 1 \quad\Longrightarrow\quad \frac{Z_n}{\sqrt{\lambda}} = 1,
	\]
	so that \(\sqrt{\lambda} = Z_n\). Therefore,
	\[
	q_n(x_{1:n}) = \frac{\gamma_n(x_{1:n})}{Z_n} = \pi_n(x_{1:n}).
	\]
	This completes the proof.
\end{proof}
\section{Sequential Importance Sampling}
\Gls*{SIS} builds upon the basic idea of \gls*{IS} by exploiting a Markov structure of the importance distribution. Instead of sampling the full trajectory at once, \gls*{SIS} extends the particle trajectories sequentially, updating the importance weights recursively. This leads to significant computational savings.

We let our importance distribution have a Markov structure, that is
\[
q_n(x_{1:n})=q_1(x_1)q_2(x_2\vert x_1)\dots q_n(x_k\vert x_{1:k-1}),
\]
and each element in the set $\{x_{1:t}\}$ we will refer to as a particle. For $n\geq 2$ define the \emph{incremental importance weight} as
\[
\alpha_n(x_{1:n}) \coloneq \frac{\gamma_n(x_{1:n})}{\gamma_{n-1}(x_{1:n-1})q_n(x_n \vert x_{1:n-1})}.
\]
The unnormalized weights can then be written recursively as 
\begin{align}
	\begin{split}
		\label{eq:unnormalized_weights_recursive}
		w_n(x_{1:n})&=\frac{\gamma_n(x_{1:n})}{q_n(x_{1:n})} \\
		&=\frac{\gamma_{n-1}(x_{1:n-1})}{q_{n-1}(x_{1:n-1})}\frac{\gamma_n(x_{1:n})}{\gamma_{n-1}(x_{1:n-1})q_n(x_n\vert x_{1:n-1})} \\
		&=w_{n-1}(x_{1:n-1})\cdot \alpha_n(x_{1:n})  \\
		&=w_1(x_1) \prod_{k=2}^n \alpha_k(x_{1:k}).
	\end{split}
\end{align}
Summarizing, the \gls*{SIS} method is described in \hyperref[algo:SIS]{Algorithm \ref*{algo:SIS}}, which has a time complexity of $O(NT)$, since the algorithm consists of two nested loops, one over the number of particles $N$, and one over the number of time steps $T$. Within the two nested loops, each operation is $O(1)$.

\begin{algorithm}[H]
	\caption{Sequential Importance Sampling (SIS)}
	\label{algo:SIS}
	\begin{algorithmic}[1]
		\State Generate particles \(x_1^{(i)}\) from  \(q_1(x_1)\) for \( i = 1, \dots, N \)
		\For{each time step \(n = 2, \dots, T\)}
		\For{each particle \( i = 1, \dots, N \)}
		\State Generate particles \(x_n^{(i)}\) from \(q_n(x_n\vert x_{1:n-1}^{(i)})\) 
		\State Compute the incremental importance weight: 
		\[
		\alpha_n^{(i)} = \frac{\gamma_n(x_{1:n}^{(i)})}{\gamma_{n-1}(x_{1:n-1}^{(i)}) q_n(x_n^{(i)} \vert x_{1:n-1}^{(i)})} 
		\] 
		\State Update the particle weight: \( w_n^{(i)} = w_{n-1}^{(i)} \cdot \alpha_n^{(i)} \)
		\EndFor
		\State Normalize the weights: \(W_n^{(i)} \leftarrow \frac{w_n^{(i)}}{\sum_{i=1}^N w_n^{(i)}} \)
		\EndFor
	\end{algorithmic}\\
	% MAKE ALGORITHM MORE LIKE PMMH? I.E. EXPLICIT INPUT OUTPUT
\end{algorithm}
We note, that this algorithm has time complexity $O(NT)$.

However, this method has a severe drawback, that the relative estimated variance $\Var(\widehat{Z}_n)/Z_n^2$ increases exponentially in $n$ even in simple examples. Example \ref{exa:rel_var_IS_explodes} illustrates this issue.
\begin{example}\label{exa:rel_var_IS_explodes}
	Consider the case where $\mathcal{X}=\mathbb{R}$ and let the density $\pi_n(x_{1:n})$ be given by
	\[
	\pi_n(x_{1:n})=\prod_{k=1}^n \pi_n(x_k)=\prod_{k=1}^n \frac{1}{\sqrt{2\pi}}\exp\Bigl(-\frac{x_k^2}{2}\Bigr).
	\]
	Thus, the unnormalized density $\gamma_n(x_{1:n})$ is given by
	\[
	\gamma_n=\prod_{k=1}^n \exp\Bigl(-\frac{x_k^2}{2}\Bigr),
	\]
	and the normalizing constant $Z_n$ is
	\[
	Z_n=(2\pi)^{n/2}.
	\]
	Ignoring that we could easily sample from $\pi_n$, we select the importance distribution $q_n$ to sample from as
	\[q_n(x_{1:n})=\prod_{k=1}^n \pi_n(x_k)=\prod_{k=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Bigl(-\frac{x_k^2}{2\sigma^2}\Bigr)\]
	and we are interested in estimating $Z_n$. Recall from \cref{thm:rel_var_Z_IS} that the relative variance of $\widehat{Z}_n$ is given by
	\[
	\frac{\Var(\widehat{Z}_n)}{Z_n^2}=\frac{1}{N}\Bigl(\int \frac{\pi_n^2(x_{1:n})}{q_n(x_{1:n})}\, dx_{1:n}-1\Bigr).
	\]
	In our case this factors over the coordinates and we have
	\begin{align*}
		\int \frac{\pi_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n}&=\prod_{k=1}^{n}\int \frac{1/(2\pi)\exp(-x^2)}{1/\sqrt{2\pi\sigma^2}\exp(-x^2/(2\sigma^2))}\, dx_{1:k} \\
		&=\biggl[\frac{\sqrt{2\pi\sigma^2}}{2\pi}\int \exp\Bigl(-x^2+\frac{x^2}{2\sigma^2}\Bigr)\, dx \biggr]^n \\
		&=\biggl[\frac{\sqrt{2\pi\sigma^2}}{2\pi}\int \exp\Bigl(-\Bigl(1-\frac{1}{\sigma^2}\Bigr)x^2\Bigr)\, dx \biggr]^n.
	\end{align*}
	The integral is finite if and only if $1-1/(2\sigma^2)>0$, that is $\sigma^2>1/2$. In this case, it is equal to 
	\[
	\int_{-\infty}^{\infty} \exp\Bigl(-\Bigl(1-\frac{1}{\sigma^2}\Bigr)x^2\Bigr)\, dx=\sqrt{\frac{\pi}{1-1/(2\sigma^2)}}.
	\]
	Thus, for $\sigma^2>1/2$ we have
	\begin{align*}
		\int \frac{\pi_n(x_{1:n})^2}{q_n(x_{1:n})}\, dx_{1:n} &=\biggl(\frac{\sqrt{2\pi\sigma^2}}{2\pi} \sqrt{\frac{\pi}{1-1/(2\sigma^2)}}\biggr)^n \\
		&=\biggl(\sqrt{\frac{\sigma^4}{2\sigma^2-1}}\biggr)^n \\
		&=\Bigl(\frac{\sigma^4}{2\sigma^2-1}\Bigr)^{n/2}.
	\end{align*}
	Finally, we can conclude that for $\sigma^2>1/2$ the relative variance becomes $\Var[\widehat{Z}_n]<\infty$ and 
	\[
	\frac{\Var[\widehat{Z}_n]}{Z_n^2}=\frac{1}{N}\biggl[\Bigl(\frac{\sigma^4}{2\sigma^2-1}\Bigr)^{n/2}-1\biggr].
	\]
	Note that for any $1/2<\sigma^2\neq 1$ we have that $\sigma^4/(2\sigma^2-1)>1$, and thus the variance increases exponentially with $n$. 
	
	For example, choosing $\sigma^2=1.2$ then we have a reasonably good importance distribution as $q_k(x_k) \approx \pi_n(x_k)$. However,  
	\[
	N\Var[\widehat{Z}_n]/Z_n^2\approx (1.103)^{n/2},
	\]
	which for $n=1000$ is roughly equal to $1.9\cdot 10^{21}$. So we would need to use $N\approx 2\cdot 10^{23}$ particles to obtain a relative variance of $0.01$.
\end{example}

\section{Resampling}
Over time, many particles receive negligible weight, leading to a situation where only a few particles dominate the estimate. This phenomenon, known as weight degeneracy, can degrade the performance of the estimator (\cite{Doucet2000}).
The idea for resampling is to get rid of particles with low weights with a high probability, so the focus is spent on high-probability regions instead of carrying forward particles with very low weights. This typically reduces the variance, see for instance Example \ref{exa:rel_var_IS_explodes_cont}. 
% Also referred to as Bootstrap filter.

The \gls*{IS} approximation \( \widehat{\pi}_n(x_{1:n}) \) of the target distribution \( \pi_n(x_{1:n}) \) is constructed using weighted samples drawn from \( q_n(x_{1:n}) \), meaning that these samples are not distributed according to \( \pi_n(x_{1:n}) \). To obtain a new set of samples that better approximates \( \pi_n(x_{1:n}) \), we employ resampling, where each particle \( X_{1:n}^{(i)} \) is selected with probability proportional to its normalized weight \( W_n^{(i)} \). 

A simple resampling strategy is multinomial resampling, where the number of offspring \( N_n^{(i)} \) assigned to each particle \( X_{1:n}^{(i)} \) follows a multinomial distribution:
\[
N_n^{(1:N)} = (N_n^{(1)},\dots, N_n^{(N)}) \sim \text{Multinomial}(N, W_n^{(1:N)}).
\]
That is, each particle is independently resampled \( N \) times, with probabilities given by the normalized weights \( W_n^{(i)} \).

After resampling, all particles are assigned equal weight, since the resampled particles are now an unweighted representation of the target distribution. Each selected particle appears with frequency \( N_n^{(i)} \), and thus the empirical distribution assigns equal mass to all \( N \) retained particles. That is, the original weighted particle approximation of the target distribution is  
\[
\hat{p}(x) = \sum_{i=1}^{N} W_n^{(i)} \delta_{X_{1:n}^{(i)}}(x),
\]  
and after resampling, the new approximation is  
\[
\hat{p}^*(x) = \sum_{i=1}^{N} \frac{N_n^{(i)}}{N} \delta_{X_{1:n}^{(i)}}(x).
\]  
Taking expectations we obtain
\[
\mathbb{E} \left[ \hat{p}^*(x) \right] = \sum_{i=1}^{N} \mathbb{E} \left[ \frac{N_n^{(i)}}{N} \right] \delta_{X_{1:n}^{(i)}}(x) = \sum_{i=1}^{N} W_n^{(i)} \delta_{X_{1:n}^{(i)}}(x) = \hat{p}(x),
\]  
showing that the resampled particle system should have equal weights to be unbiased. 

Using the recursive structure of the unnormalized weights from \cref{eq:unnormalized_weights_recursive} a natural way to estimate 
$Z_n$ is to define 
\[
\widetilde{Z}_1\coloneq \frac{1}{N}\sum_{i=1}^N w_1(x_1^{i}),
\]
and for $n\geq 2$ estimate $Z_n$ recursively by 
\begin{equation}
	\widetilde{Z}_n \coloneq\widetilde{Z}_{n-1}{\alpha}_n^{\text{MC}}, \label{eq:est_Z_SIS}
\end{equation}
where ${\alpha}_n^{\text{MC}}$ is the standard MC estimate of $\alpha_n$. 

A generic \gls*{SISR} algorithm is given in \hyperref[algo:SISR]{Algorithm \ref*{algo:SISR}}. The time complexity of this algorithm is \(O(NT)\), since it consists of two nested loops, one over the number of particles \(N\), and one over the number of time steps \(T\). Within the nested loops, each operation is \(O(1)\). The resampling step, which involves drawing \(N\) indices according to the particle weights, is done within the \(T\)-loop and has a time complexity of \(O(N)\). Therefore, the overall time complexity is \(O(NT + NT) = O(NT)\).

\begin{algorithm}[H]
	\caption{Sequential Importance Sampling with Resampling (SISR)}
	\label{algo:SISR}
	\begin{algorithmic}[1]
		\State Generate particles \(x_1^{(i)}\) from \( q_1(x_1)\) for \( i = 1, \dots, N \)
		\For{each time step \(n = 2, \dots, T\)}
		\For{each particle \( i = 1, \dots, N \)}
		\State Generate particles \(x_n^{(i)}\) from \(q_n(x_n\vert x_{1:n-1}^{(i)})\) 
		\State Compute the incremental importance weight: 
		\[
		\alpha_n^{(i)} = \frac{\gamma_n(x_{1:n}^{(i)})}{\gamma_{n-1}(x_{1:n-1}^{(i)}) q_n(x_n^{(i)} \vert x_{1:n-1}^{(i)})} 
		\] 
		\State Update the particle weight: \( w_n^{(i)} = w_{n-1}^{(i)} \cdot \alpha_n^{(i)} \)
		\EndFor
		\State Normalize the weights: \(W_n^{(i)} \leftarrow \frac{w_n^{(i)}}{\sum_{i=1}^N w_n^{(i)}} \)
		\State Draw \(N\) indices \(\{a^{(i)}\}_{i=1}^N\) from \(\{1,\dots,N\}\) according to the probabilities \(\{W_n^{(i)}\}\)
		\State Set \( X_{1:n}^{(i)} \leftarrow X_{1:n}^{(a^{(i)})} \) for all \( i \)
		\State Reset weights: $w_n^{(i)}=1$
		\EndFor
	\end{algorithmic}
	% MAKE ALGORITHM MORE LIKE PMMH? I.E. EXPLICIT INPUT OUTPUT
\end{algorithm}
While resampling effectively eliminates low-weight particles, it also causes many distinct trajectories to vanish over successive iterations. In effect, resampling resets the system by providing a reliable approximation of the current state's marginal distribution, albeit at the cost of losing detailed ancestral information. The deeper problem of weight degeneracy is that trying to represent a high-dimensional distribution with a finite number of samples will inevitably fail. 

An alternative would be to increase the number of particles at each iteration; however, this approach quickly becomes infeasible due to the exponential growth in the required number of particles (\cite{Doucet}).

Now we provide a result similar to \cref{thm:rel_var_Z_IS} for the case with resampling. 

\begin{theorem}[Relative Asymptotic Variance with Resampling]
	\label{thm:rel_asym_var_resampling}
	The relative asymptotic variance of the \gls*{IS} estimate of the normalizing constant \(Z_n\) with resampling at every time step is
	\[
	\frac{\Var(\widetilde{Z}_n)}{Z_n^2}=\frac{1}{N}\left[\left(\int 	\frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right)+\sum_{k=2}^n\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right)\right].
	\]
\end{theorem}
A proof is omitted but follows from the Feynmanâ€“Kac framework; see, e.g., Chapter 9 in \cite{moral2004feynman} for a proof.

Notably, comparing \cref{thm:rel_asym_var_resampling} with \cref{thm:rel_var_Z_IS} reveals that while resampling introduces additional variance, the resampling has a resetting property of the systems. Thus, we get that the associated errors accumulate linearly rather than multiplicatively. This becomes important when the dimension becomes large. We continue Example \ref{exa:rel_var_IS_explodes} using \cref{thm:rel_asym_var_resampling} to highlight this.
\begin{example}[Example \ref{exa:rel_var_IS_explodes} continued]
	\label{exa:rel_var_IS_explodes_cont}
	Using \cref{thm:rel_asym_var_resampling} and using the derivation done in Example \ref{exa:rel_var_IS_explodes} we have that it is finite for $\sigma^2>1/2$ and the relative variance using resampling at every time step is approximately equal to
	\begin{align*}
		\frac{\Var(\widetilde{Z}_n)}{Z_n^2}&\approx\frac{1}{N}\left[\left(\int 	\frac{\pi_1^2(x_1)}{q_1(x_1)}\,dx_1-1\right)+\sum_{k=2}^n\left(\int \frac{\pi_k^2(x_{1:k})}{\pi_{k-1}(x_{1:k-1})\,q_k(x_k\vert x_{1:k-1})}\,dx_{k-1:k}-1\right)\right] \\
		&=\frac{n}{N}\left[\left(\frac{\sigma^4}{2\sigma^2-1}^{1/2}\right)-1\right]
	\end{align*}
	which is linear in $n$ in contrast to the exponential growth of the \gls*{IS} estimate of the relative variance. If we again select $\sigma^2=1.2$ then to obtain a relative variance of $0.01$ we only need $N\approx 10^4$ particles instead of the $N\approx 2\cdot 10^{23}$ particles that were needed for the IS estimate to obtain the same precision. That is, we obtain an improvement by 19 orders of magnitude.
\end{example} 
This setup favors SMC massively since the density $\pi_n(x_{1:n})$) factorizes. A more realistic example is given in Example \ref{exa:SSM_known_theta}.

\subsection{Reducing variance of Resampling}
While resampling helps mitigate weight degeneracy by focusing computational effort on high-probability regions, it introduces additional variance into the algorithm. We describe two distinct techniques that can reduce this extra variance, thereby improving the overall efficiency of the \gls*{SISR} algorithm. Notably, these approaches are complementary and can be applied simultaneously.

We can reduce the variance introduced during resampling by changing the sampling scheme. Several methods exists, but we will focus on stratified resampling, a method often used in survey sampling \cite{kiderlen2022survey}. In stratified resampling, the interval $[0,1]$ is divided into $N$ equal strata, and one uniform random number is drawn from each sub-interval. That is, for $i=1,\dots,N$, we draw 
\[
u_i \sim \text{Uniform}\left(\frac{i-1}{N}, \frac{i}{N}\right).
\]
Each $u_i$ is then used to select a particle based on the cumulative normalized weights. In \cite{douc2005comparisonresamplingschemesparticle} it was shown that stratified resampling always gives lower variance than multinomial resampling. 

% Add systematic resampling?

Another way to reduce the variance of the \gls*{SISR} algorithm is to only do the resampling step when we have many particles with low weights. We will call this \gls*{SISAR}.
A common metric used to decide when to trigger a resampling step is the \emph{effective sample size} (ESS), first introduced by \cite{Liu}. The ESS at time $n$ is defined as
\[
\text{ESS}_n \coloneq \frac{1}{\sum_{i=1}^N \left(W_n^{(i)}\right)^2}\,.
\]
ESS can take a value between $1$ and $N$. When the ESS falls below a predetermined threshold, $N_\tau$, often chosen as $N_\tau=N/2$, it is an indication that most of the weight is carried by only a few particles, and resampling is then warranted.

Thus, the \gls*{SISAR} Algorithm is just the \gls*{SISR} Algorithm described in \cref{algo:SISR} where the resampling is only done when the resampling condition is met. A generic algorithm incorporating adaptive resampling is described in \hyperref[algo:SISAR]{Algorithm \ref*{algo:SISAR}}. The time complexity remains $O(NT)$.
\begin{algorithm}[H]
	\caption{Sequential Importance Sampling with Adaptive Resampling (SISAR)}
	\label{algo:SISAR}
	\begin{algorithmic}[1]
		\State Generate particles \(x_1^{(i)}\) from \( q_1(x_1)\) for \( i = 1, \dots, N \)
		\For{each time step \(n = 2, \dots, T\)}
		\For{each particle \( i = 1, \dots, N \)}
		\State Generate particles \(x_n^{(i)}\) from \(q_n(x_n\vert x_{1:n-1}^{(i)})\) 
		\State Compute the incremental importance weight: 
		\[
		\alpha_n^{(i)} = \frac{\gamma_n(x_{1:n}^{(i)})}{\gamma_{n-1}(x_{1:n-1}^{(i)}) q_n(x_n^{(i)} \vert x_{1:n-1}^{(i)})} 
		\] 
		\State Update the particle weight: \( w_n^{(i)} = w_{n-1}^{(i)} \cdot \alpha_n^{(i)} \)
		\EndFor
		\State Normalize the weights: \(W_n^{(i)} \leftarrow \frac{w_n^{(i)}}{\sum_{i=1}^N w_n^{(i)}} \)
		\If{Resampling condition is met}
		\State Draw \(N\) indices \(\{a^{(i)}\}_{i=1}^N\) from \(\{1,\dots,N\}\) according to the probabilities \(\{W_n^{(i)}\}\)
		\State Set \( x_{1:n}^{(i)} \leftarrow x_{1:n}^{(a^{(i)})} \) for all \( i \)
		\State Reset weights: $w_n^{(i)}=1$
		\EndIf
		\EndFor
	\end{algorithmic}
	% MAKE ALGORITHM MORE LIKE PMMH? I.E. EXPLICIT INPUT OUTPUT
\end{algorithm}
\noindent Again, we can at any step $n$ estimate $Z_n$ by \cref{eq:est_Z_SIS}. 
